{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd162df",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cc2d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\regis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 0.1 — Téléchargement NLTK (une seule fois)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95431f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:481: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:338: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os, json, itertools\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "from langdetect import detect\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63def28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch CUDA OK : NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 0.3 — Vérification GPU PyTorch\n",
    "assert torch.cuda.is_available(), \"CUDA non disponible !\"\n",
    "print(\"✅ PyTorch CUDA OK :\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f6d69",
   "metadata": {},
   "source": [
    "# 1. configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b5e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR    = Path(\"data\");     BASE_DIR.mkdir(exist_ok=True)\n",
    "RAW_PDF_DIR = BASE_DIR / \"raw_pdf\"; RAW_PDF_DIR.mkdir(exist_ok=True)\n",
    "LLM_MODEL   = \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b19b1",
   "metadata": {},
   "source": [
    "- https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "- https://chatgpt.com/c/6877401e-7b98-8000-abd1-c8a22773f439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826184f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_cfg = {\n",
    "  \"embedding_models\": [\"all-mpnet-base-v2\",],\n",
    "  \"chunk_sizes\":      [512,768,1024],\n",
    "  \"chunk_overlaps\":   [128],\n",
    "  \"top_k_list\":       [10,15,20],\n",
    "  \"similarities\":     [\"cosine\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cad34bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global config written to config.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\",\"w\") as f: json.dump(general_cfg,f,indent=2)\n",
    "print(\"✅ Global config written to config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916e9c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedders = {m: SentenceTransformer(m,device=\"cuda\")\n",
    "             for m in general_cfg[\"embedding_models\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396dfe",
   "metadata": {},
   "source": [
    "# 2. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5f1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_exp_dir(base:Path)->Path:\n",
    "    idxs=[int(d.name.split(\"_\")[1])for d in base.iterdir()\n",
    "          if d.is_dir() and d.name.startswith(\"exp_\")]\n",
    "    nxt= max(idxs)+1 if idxs else 0\n",
    "    p=base/f\"exp_{nxt}\"; p.mkdir(exist_ok=True)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1f25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(p:Path)->str:\n",
    "    doc=fitz.open(str(p))\n",
    "    return \"\\n\".join(pg.get_text() for pg in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df4f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text,cs,ov)->list[dict]:\n",
    "    sents=sent_tokenize(text)\n",
    "    chunks=[]; curr=[]; cnt=0\n",
    "    for s in sents:\n",
    "        curr.append(s); cnt+=len(s)\n",
    "        if cnt>=cs:\n",
    "            txt=\" \".join(curr)\n",
    "            chunks.append({\"text\":txt,\"lang\":detect(txt)})\n",
    "            curr=curr[-ov:]; cnt=sum(len(x) for x in curr)\n",
    "    if curr:\n",
    "        txt=\" \".join(curr)\n",
    "        chunks.append({\"text\":txt,\"lang\":detect(txt)})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6141fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_torch_index(embs:np.ndarray,use_cosine:bool)->torch.Tensor:\n",
    "    t=torch.from_numpy(embs).to(\"cuda\")\n",
    "    return F.normalize(t,2,1) if use_cosine else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a95b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_torch_lang(tensor_index:torch.Tensor,\n",
    "                    q_embs:torch.Tensor,\n",
    "                    langs:list[str],\n",
    "                    top_k:int)->(float,float):\n",
    "    recalls=[]; mrrs=[]\n",
    "    for i,qv in enumerate(q_embs):\n",
    "        lang=langs[i]\n",
    "        mask = torch.tensor([l==lang for l in langs], device=\"cuda\")\n",
    "        inds = retrieve_topk_lang(tensor_index, qv, mask, top_k)\n",
    "        rec = float(i in inds)\n",
    "        if rec:\n",
    "            rank=int((inds==i).nonzero()[0])+1; mrr=1.0/rank\n",
    "        else:\n",
    "            mrr=0.0\n",
    "        recalls.append(rec); mrrs.append(mrr)\n",
    "    return float(np.mean(recalls)), float(np.mean(mrrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ba3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_torch_lang(tensor_index:torch.Tensor,\n",
    "                    q_embs:torch.Tensor,\n",
    "                    langs:list[str],\n",
    "                    top_k:int)->(float,float):\n",
    "    recalls=[]; mrrs=[]\n",
    "    # q_embs: Tensor (n_q, dim) already on GPU\n",
    "    for i,qv in enumerate(q_embs):\n",
    "        lang=langs[i]\n",
    "        mask = torch.tensor([l==lang for l in langs], device=\"cuda\")\n",
    "        inds = retrieve_topk_lang(tensor_index, qv, mask, top_k)\n",
    "        rec = float(i in inds)\n",
    "        if rec:\n",
    "            rank=int((inds==i).nonzero()[0])+1; mrr=1.0/rank\n",
    "        else: mrr=0.0\n",
    "        recalls.append(rec); mrrs.append(mrr)\n",
    "    return float(np.mean(recalls)), float(np.mean(mrrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1036c6",
   "metadata": {},
   "source": [
    "# 3. experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d3a6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "cfg = general_cfg\n",
    "pdfs = list(RAW_PDF_DIR.glob(\"*.pdf\"))\n",
    "if not pdfs:\n",
    "    raise FileNotFoundError(\"Aucun PDF trouvé dans data/raw_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61703ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAResponse(BaseModel):\n",
    "    question: str = Field(description=\"A single clear question\")\n",
    "    answer: str = Field(description=\"The answer to that question\")\n",
    "\n",
    "# Setup the output parser\n",
    "json_parser = PydanticOutputParser(pydantic_object=QAResponse)\n",
    "\n",
    "# Create a cleaner prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=(\n",
    "        \"Based on the text below ({lang}), generate EXACTLY ONE question-and-answer pair.\\n\\n\"\n",
    "        \"Text:\\n{doc}\\n\\n\"\n",
    "        \"Respond ONLY with a JSON object containing a question and answer extracted from the text.\\n\"\n",
    "        \"{format_instructions}\\n\"\n",
    "    ),\n",
    "    input_variables=[\"lang\", \"doc\"],\n",
    "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Initialize the chain\n",
    "llm = ChatOllama(model=LLM_MODEL)\n",
    "qag = LLMChain(llm=llm, prompt=prompt_template, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Exp=exp_0 | model=all-mpnet-base-v2 | chunk=512/128 | topk=10 | sim=cosine\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dbd6346c79400fb4bc8c152ab5ee3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\regis\\AppData\\Local\\Temp\\ipykernel_19104\\3760587222.py:47: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qag = LLMChain(llm=llm, prompt=prompt_template, output_parser=json_parser, verbose=False)\n",
      "C:\\Users\\regis\\AppData\\Local\\Temp\\ipykernel_19104\\3760587222.py:52: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa_resp: QAResponse = qag({\"lang\": lang, \"doc\": row[\"text\"]})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'\\n  \"question\"'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     51\u001b[0m     lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFR\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 52\u001b[0m     qa_resp: QAResponse \u001b[38;5;241m=\u001b[39m \u001b[43mqag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     questions\u001b[38;5;241m.\u001b[39mappend(qa_resp\u001b[38;5;241m.\u001b[39mquestion)\n\u001b[0;32m     54\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(qa_resp\u001b[38;5;241m.\u001b[39manswer)\n",
      "File \u001b[1;32mc:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     emit_warning()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\langchain\\chains\\base.py:386\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    384\u001b[0m }\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\langchain\\chains\\base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\langchain\\chains\\base.py:155\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     inputs,\n\u001b[0;32m    151\u001b[0m     run_id,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    164\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\regis\\OneDrive\\Documents\\aiclinique\\chatbot-seizure2\\venv\\lib\\site-packages\\langchain\\chains\\base.py:287\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    285\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing some input keys: {'\\n  \"question\"'}"
     ]
    }
   ],
   "source": [
    "for emb_model, cs, ov, tk, sim in itertools.product(\n",
    "    cfg[\"embedding_models\"],\n",
    "    cfg[\"chunk_sizes\"],\n",
    "    cfg[\"chunk_overlaps\"],\n",
    "    cfg[\"top_k_list\"],\n",
    "    cfg[\"similarities\"],\n",
    "):\n",
    "    exp_dir = get_next_exp_dir(BASE_DIR)\n",
    "    print(f\"\\n▶ Exp={exp_dir.name} | model={emb_model} | chunk={cs}/{ov} | topk={tk} | sim={sim}\")\n",
    "    mlflow.start_run(run_name=exp_dir.name)\n",
    "    mlflow.log_params({\"model\": emb_model, \"cs\": cs, \"ov\": ov, \"top_k\": tk, \"sim\": sim})\n",
    "\n",
    "    # 3.1 Chunking\n",
    "    flat = []\n",
    "    for pdf in pdfs:\n",
    "        text = extract_text(pdf)\n",
    "        try:\n",
    "            doc_lang = detect(text)\n",
    "        except:\n",
    "            doc_lang = \"en\"\n",
    "        chunks = chunk_text(text, cs, ov)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            flat.append({\n",
    "                \"doc\": pdf.stem,\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"lang\": doc_lang,\n",
    "            })\n",
    "    mlflow.log_metric(\"num_chunks\", len(flat))\n",
    "\n",
    "    # Sauvegarde des chunks\n",
    "    (exp_dir / \"chunks\").mkdir(exist_ok=True)\n",
    "    for item in flat:\n",
    "        fname = f\"{item['doc']}_chunk_{item['chunk_id']:04d}.txt\"\n",
    "        (exp_dir / \"chunks\" / fname).write_text(item[\"text\"], encoding=\"utf-8\")\n",
    "\n",
    "    # 3.2 Embeddings & Index\n",
    "    embedder = embedders[emb_model]\n",
    "    texts = [item[\"text\"] for item in flat]\n",
    "    emb_t = embedder.encode(texts, batch_size=64, convert_to_tensor=True, device=\"cuda\", show_progress_bar=True)\n",
    "    arr = emb_t.cpu().numpy().astype(\"float32\")\n",
    "    tensor_index = build_torch_index(arr, sim == \"cosine\")\n",
    "\n",
    "    # 3.3 Q/A Generation (JSON via Pydantic)\n",
    "    df = pd.DataFrame(flat)\n",
    "\n",
    "    questions, answers = [], []\n",
    "    for i, row in df.iterrows():\n",
    "        lang = \"FR\" if row[\"lang\"].startswith(\"fr\") else \"EN\"\n",
    "        result = qag({\"lang\": lang, \"doc\": row[\"text\"]})\n",
    "        # Parse the output text to extract the JSON\n",
    "        try:\n",
    "            parsed_output = json_parser.parse(result[\"text\"])\n",
    "            questions.append(parsed_output.question)\n",
    "            answers.append(parsed_output.answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing output for row {i}: {e}\")\n",
    "            questions.append(\"\")\n",
    "            answers.append(\"\")\n",
    "    df[\"question\"] = questions\n",
    "    df[\"reference_answer\"] = answers\n",
    "\n",
    "    csv_path = exp_dir / \"questions.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    mlflow.log_artifact(str(csv_path))\n",
    "\n",
    "    # 3.4 Retrieval evaluation\n",
    "    mean_recall, mean_mrr = eval_torch_lang(tensor_index, emb_t, df['lang'].tolist(), tk)\n",
    "    mlflow.log_metric(f\"mean_recall@{tk}\", mean_recall)\n",
    "    mlflow.log_metric(f\"mean_mrr@{tk}\", mean_mrr)\n",
    "\n",
    "    # 3.5 Fin\n",
    "    config = {\"model\": emb_model, \"cs\": cs, \"ov\": ov, \"top_k\": tk, \"sim\": sim}\n",
    "    with open(exp_dir / \"config.json\", \"w\") as cfg_file:\n",
    "        json.dump(config, cfg_file, indent=2)\n",
    "    mlflow.log_artifact(str(exp_dir / \"config.json\"))\n",
    "    mlflow.end_run()  # termine l'exp\n",
    "    print(\"✔ Expérience terminée avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b4187",
   "metadata": {},
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
