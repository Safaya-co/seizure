# app.py - Backend Flask avec historique JSON pour assistant m√©dical

from flask import Flask, request, jsonify, render_template
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from dotenv import load_dotenv
from openai import OpenAI
import torch
import pickle
import os
import json
from datetime import datetime

# --- Charger variables d'environnement depuis fichier .env ---
load_dotenv()

# --- Configuration Flask ---
app = Flask(__name__)

# --- Chemins absolus ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RETRIEVAL_FILE = os.path.join(BASE_DIR, "data", "retrieval_data_bge_v15.pkl")
EMBEDDINGS_FILE = os.path.join(BASE_DIR, "data", "embeddings_bge_v15.pt")
HISTORY_FILE = os.path.join(BASE_DIR, "chat_history.json")

# --- Configuration API Mistral via OpenAI SDK ---
api_key = os.getenv("MISTRAL_API_KEY") or os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("Aucun API key trouv√©. D√©finir MISTRAL_API_KEY ou OPENAI_API_KEY dans .env.")

# CORRECTION: Configuration correcte pour Mistral
client = OpenAI(
    api_key=api_key,
    base_url="https://api.mistral.ai/v1"  # Utiliser base_url au lieu de api_base
)
print(f"üîë Cl√© API charg√©e (masqu√©e): {api_key[:6]}***{api_key[-4:]}")

# --- Test de connexion √† l'API Mistral ---
try:
    models = client.models.list()
    count = len(models.data) if hasattr(models, 'data') else len(models)
    print(f"‚úÖ Connexion √† l'API Mistral r√©ussie, {count} mod√®les disponibles.")
    # Afficher les mod√®les disponibles
    if hasattr(models, 'data'):
        print("Mod√®les disponibles:")
        for model in models.data[:5]:  # Afficher les 5 premiers
            print(f"  - {model.id}")
except Exception as e:
    print(f"‚ùå √âchec de la connexion √† l'API Mistral: {e}")

# --- Chargement des mod√®les locaux ---
print("Loading embedding model...")
embedding_model = SentenceTransformer("BAAI/bge-base-en-v1.5", device="cpu")
print("Loading reranker model...")
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# --- Chargement des donn√©es de retrieval ---
with open(RETRIEVAL_FILE, "rb") as f:
    retrieval_data = pickle.load(f)
all_chunks = retrieval_data.get("chunk", retrieval_data.get("chunks", [])).fillna("").astype(str).tolist()
chunk_embeddings = torch.load(EMBEDDINGS_FILE, map_location="cpu")

# --- Fonctions auxiliaires ---
def get_top_k_chunks(query, k=20, rerank_k=10):
    """Retourne les top-k chunks rerank√©s pour une question."""
    q_emb = embedding_model.encode(query, convert_to_tensor=True, normalize_embeddings=True)
    scores = util.cos_sim(q_emb, chunk_embeddings)[0]
    top_idx = torch.topk(scores, k=k).indices.tolist()
    candidates = [all_chunks[i] for i in top_idx]
    rerank_inputs = [(query, c) for c in candidates]
    rerank_scores = reranker.predict(rerank_inputs)
    ranked = sorted(zip(candidates, rerank_scores), key=lambda x: x[1], reverse=True)
    return [text for text, _ in ranked[:rerank_k]]


def classify_question_type(question):
    """D√©termine si une question n√©cessite le RAG ou peut √™tre r√©pondue directement."""
    
    # Mots-cl√©s indiquant une question m√©dicale g√©n√©rale (pas besoin de RAG)
    general_keywords = [
        "bonjour", "salut", "hello", "comment allez-vous", "comment √ßa va",
        "qui √™tes-vous", "que faites-vous", "pouvez-vous m'aider",
        "qu'est-ce que", "d√©finition", "expliquer", "c'est quoi",
        "merci", "au revoir", "goodbye", "√† bient√¥t"
    ]
    
    # Mots-cl√©s indiquant une question sp√©cialis√©e (besoin de RAG)
    specialized_keywords = [
        "crise", "√©pilepsie", "convulsion", "seizure", "eeg", "√©lectroenc√©phalogramme",
        "diagnostic", "traitement", "m√©dicament", "anti√©pileptique", "protocole",
        "sympt√¥me", "signe", "manifestation", "patient", "cas clinique",
        "dosage", "posologie", "effet secondaire", "contre-indication"
    ]
    
    question_lower = question.lower()
    
    # V√©rifier les mots-cl√©s g√©n√©raux
    general_score = sum(1 for keyword in general_keywords if keyword in question_lower)
    
    # V√©rifier les mots-cl√©s sp√©cialis√©s
    specialized_score = sum(1 for keyword in specialized_keywords if keyword in question_lower)
    
    # Questions tr√®s courtes (moins de 10 mots) sont souvent g√©n√©rales
    word_count = len(question.split())
    
    # Logique de classification
    if general_score > 0 and specialized_score == 0:
        return "general", 0.9
    elif specialized_score > 0:
        return "specialized", min(0.9, 0.5 + (specialized_score * 0.1))
    elif word_count < 5:
        return "general", 0.7
    else:
        # Par d√©faut, traiter comme sp√©cialis√© mais avec moins de confiance
        return "specialized", 0.6


def get_conversation_context(current_question, max_context_items=5):
    """R√©cup√®re le contexte de conversation r√©cent pour maintenir la m√©moire."""
    history = load_history()
    
    if not history:
        return None, False
    
    # Prendre les N derni√®res interactions (exclure la question actuelle si elle est d√©j√† sauv√©e)
    recent_history = history[-max_context_items:]
    
    # V√©rifier s'il y a des r√©f√©rences contextuelles dans la question actuelle
    contextual_indicators = [
        "comme vous avez dit", "comme mentionn√©", "en r√©f√©rence √†", "suite √†",
        "concernant votre r√©ponse", "√† propos de", "dans le cas pr√©c√©dent",
        "il", "elle", "cela", "√ßa", "ce", "cette", "celui", "celle",
        "et aussi", "√©galement", "de plus", "en plus"
    ]
    
    has_contextual_reference = any(indicator in current_question.lower() 
                                 for indicator in contextual_indicators)
    
    # Construire le contexte de conversation
    if recent_history and (has_contextual_reference or len(recent_history) >= 2):
        context_parts = []
        for i, entry in enumerate(recent_history):
            context_parts.append(f"[Interaction {i+1}]")
            context_parts.append(f"Question: {entry['question']}")
            # R√©sumer la r√©ponse si elle est longue
            answer = entry['answer']
            if len(answer) > 200:
                answer = answer[:200] + "..."
            context_parts.append(f"R√©ponse: {answer}")
            context_parts.append("")  # Ligne vide
        
        return "\n".join(context_parts), has_contextual_reference
    
    return None, has_contextual_reference


def build_prompt_with_memory(query, top_chunks, is_grounded=True, conversation_context=None):
    """Construit le prompt avec contexte de conversation si disponible."""
    
    # Prompt de base selon le type (grounded ou non)
    if is_grounded:
        base_instruction = (
            "R√©ponds uniquement avec les informations fournies dans les contextes ci-dessous. "
            "Si l'information n'est pas pr√©sente, dis 'Je ne sais pas'."
        )
    else:
        base_instruction = (
            "Les contextes suivants contiennent peu d'informations pertinentes pour cette question. "
            "R√©ponds en te basant sur tes connaissances m√©dicales g√©n√©rales, tout en restant prudent et en recommandant une consultation m√©dicale si n√©cessaire."
        )
    
    # Contexte documentaire
    if is_grounded:
        doc_context = "\n\n".join([f"[Doc {i+1}] {chunk.strip()}" for i, chunk in enumerate(top_chunks)])
    else:
        doc_context = "\n\n".join([f"[Doc {i+1}] {chunk.strip()}" for i, chunk in enumerate(top_chunks[:3])])
    
    # Construire le prompt avec ou sans m√©moire conversationnelle
    if conversation_context:
        return f"""{base_instruction}

CONTEXTE DE CONVERSATION R√âCENTE :
{conversation_context}

CONTEXTES DOCUMENTAIRES :
{doc_context}

QUESTION ACTUELLE : {query}

Instructions suppl√©mentaires :
- Tiens compte de l'historique de conversation pour donner une r√©ponse coh√©rente
- Si la question fait r√©f√©rence √† une discussion pr√©c√©dente, utilise ce contexte
- Reste dans le domaine m√©dical et sp√©cialis√© en d√©tection de crises

R√©ponse :"""
    else:
        return f"""{base_instruction}

CONTEXTES DOCUMENTAIRES :
{doc_context}

Question : {query}

R√©ponse :"""


def call_mistral_with_memory(prompt, is_grounded=True, has_memory_context=False):
    """Appel √† Mistral avec gestion de la m√©moire conversationnelle."""
    try:
        # Adapter le message syst√®me selon la pr√©sence de m√©moire
        if has_memory_context:
            if is_grounded:
                system_message = (
                    "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises. "
                    "Tu as acc√®s √† l'historique de la conversation et peux t'y r√©f√©rer pour donner des r√©ponses coh√©rentes. "
                    "Utilise ce contexte pour personnaliser tes r√©ponses."
                )
            else:
                system_message = (
                    "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises. "
                    "Tu as acc√®s √† l'historique de la conversation. "
                    "IMPORTANT: Tu dois commencer ta r√©ponse par '‚ö†Ô∏è ATTENTION: Cette r√©ponse ne se base pas avec certitude sur la documentation fournie. ' "
                    "puis donner une r√©ponse bas√©e sur tes connaissances g√©n√©rales et le contexte conversationnel."
                )
        else:
            # Messages syst√®me originaux si pas de contexte m√©moire
            if is_grounded:
                system_message = "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises."
            else:
                system_message = (
                    "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises. "
                    "IMPORTANT: Tu dois commencer ta r√©ponse par '‚ö†Ô∏è ATTENTION: Cette r√©ponse ne se base pas avec certitude sur la documentation fournie. ' "
                    "puis donner une r√©ponse bas√©e sur tes connaissances g√©n√©rales tout en restant prudent."
                )
        
        response = client.chat.completions.create(
            model="mistral-medium-2505",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=512,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Erreur API Mistral (avec m√©moire): {e}")
        return f"[ERREUR] {e}"


def call_mistral_without_rag_with_memory(question, conversation_context=None):
    """R√©pond √† une question g√©n√©rale avec m√©moire conversationnelle."""
    try:
        # Construire le prompt avec contexte si disponible
        if conversation_context:
            prompt = f"""CONTEXTE DE CONVERSATION R√âCENTE :
{conversation_context}

QUESTION ACTUELLE : {question}

Instructions :
- Tiens compte de l'historique pour donner une r√©ponse coh√©rente
- R√©ponds de mani√®re concise et professionnelle
- Pour les questions m√©dicales sp√©cifiques, recommande une consultation m√©dicale

R√©ponse :"""
            system_message = (
                "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises. "
                "Tu as acc√®s √† l'historique de conversation et peux t'y r√©f√©rer pour maintenir la coh√©rence."
            )
        else:
            prompt = question
            system_message = (
                "Tu es un assistant m√©dical sp√©cialis√© en d√©tection de crises. "
                "R√©ponds de mani√®re concise et professionnelle. "
                "Pour les questions m√©dicales sp√©cifiques, recommande toujours une consultation m√©dicale."
            )
        
        response = client.chat.completions.create(
            model="mistral-medium-2505",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=256,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Erreur API Mistral (sans RAG avec m√©moire): {e}")
        return f"[ERREUR] {e}"


def generate_short_summary(long_answer):
    """G√©n√®re un r√©sum√© court d'une r√©ponse longue."""
    # Seuil pour consid√©rer une r√©ponse comme longue
    if len(long_answer.split()) < 50:
        return None  # Pas besoin de r√©sum√© pour les r√©ponses courtes
    
    try:
        response = client.chat.completions.create(
            model="mistral-medium-2505",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Tu dois r√©sumer le texte m√©dical fourni en 2-3 phrases maximum. "
                        "Garde les informations essentielles et les recommandations importantes. "
                        "Commence par 'En r√©sum√©:'"
                    )
                },
                {
                    "role": "user",
                    "content": f"R√©sume ce texte m√©dical:\n\n{long_answer}"
                },
            ],
            temperature=0.2,
            max_tokens=150,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Erreur g√©n√©ration r√©sum√©: {e}")
        return None


def compute_similarity(answer, context):
    """Calcule la similarit√© cosinus entre r√©ponse et contexte."""
    emb_ans = embedding_model.encode(answer, convert_to_tensor=True, normalize_embeddings=True)
    emb_ctx = embedding_model.encode(context, convert_to_tensor=True, normalize_embeddings=True)
    return util.cos_sim(emb_ans, emb_ctx).item()


def save_to_history(question, answer, score, grounded, question_type, used_rag, memory_used):
    """Sauvegarde l'interaction dans le fichier JSON d'historique."""
    entry = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
        "cosine_score": round(score, 3),
        "grounded": grounded,
        "confidence_level": "haute" if grounded else "faible",
        "has_warning": answer.startswith("‚ö†Ô∏è"),
        "question_type": question_type,
        "used_rag": used_rag,
        "memory_used": memory_used,
        "answer_length": len(answer.split())
    }
    history = load_history()
    history.append(entry)
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)


def load_history():
    """Charge l'historique JSON si existant, sinon renvoie []"""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

# --- Routes Flask ---
@app.route("/", methods=["GET"])
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json() or {}
    question = data.get("message", "").strip()
    if not question:
        return jsonify({"response": "Aucune question fournie."})

    # √âTAPE 1: Classification de la question
    question_type, classification_confidence = classify_question_type(question)
    print(f"üìù Question classifi√©e: {question_type} (confiance: {classification_confidence})")
    
    # √âTAPE 2: R√©cup√©ration du contexte conversationnel
    conversation_context, has_contextual_ref = get_conversation_context(question)
    memory_used = conversation_context is not None
    print(f"üß† M√©moire: {'Utilis√©e' if memory_used else 'Non utilis√©e'} | R√©f√©rence contextuelle: {has_contextual_ref}")
    
    # √âTAPE 3: Traitement selon le type de question
    if question_type == "general":
        # R√©ponse directe sans RAG mais avec m√©moire possible
        answer = call_mistral_without_rag_with_memory(question, conversation_context)
        cosine_score = 0.0  # Pas de score RAG pour les questions g√©n√©rales
        grounded = False
        used_rag = False
        short_summary = None
        
    else:
        # Utiliser le RAG pour les questions sp√©cialis√©es avec m√©moire
        used_rag = True
        top_chunks = get_top_k_chunks(question)
        context_concat = " ".join(top_chunks)
        
        # Calculer la similarit√© AVANT de g√©n√©rer la r√©ponse
        temp_similarity = compute_similarity(question, context_concat)
        grounded = temp_similarity >= 0.4
        
        # Construire le prompt adapt√© avec m√©moire
        prompt = build_prompt_with_memory(question, top_chunks, is_grounded=grounded, conversation_context=conversation_context)
        answer = call_mistral_with_memory(prompt, is_grounded=grounded, has_memory_context=memory_used)
        
        # Calculer la vraie similarit√© avec la r√©ponse finale
        cosine_score = compute_similarity(answer, context_concat)
        final_grounded = cosine_score >= 0.7
        
        # Ajuster le statut si n√©cessaire
        if not grounded and not answer.startswith("‚ö†Ô∏è"):
            answer = f"‚ö†Ô∏è ATTENTION: Cette r√©ponse ne se base pas avec certitude sur la documentation fournie.\n\n{answer}"
        
        grounded = final_grounded
        
        # √âTAPE 4: G√©n√©rer un r√©sum√© court si la r√©ponse est longue
        short_summary = generate_short_summary(answer)
    
    # Sauvegarde dans l'historique
    save_to_history(question, answer, cosine_score, grounded, question_type, used_rag, memory_used)
    
    # Pr√©parer la r√©ponse
    response_data = {
        "response": answer,
        "cosine_score": round(cosine_score, 3),
        "grounded_in_docs": grounded,
        "confidence_level": "haute" if grounded else "faible",
        "question_type": question_type,
        "classification_confidence": round(classification_confidence, 2),
        "used_rag": used_rag,
        "memory_used": memory_used,
        "has_contextual_reference": has_contextual_ref,
        "warning_displayed": answer.startswith("‚ö†Ô∏è")
    }
    
    # Ajouter le r√©sum√© court s'il existe
    if short_summary:
        response_data["short_summary"] = short_summary
        response_data["has_short_summary"] = True
    else:
        response_data["has_short_summary"] = False
    
    return jsonify(response_data)

@app.route("/history", methods=["GET"])
def history():
    return jsonify(load_history())

# --- Lancement de l'application ---
if __name__ == "__main__":
    app.run(debug=True, port=5001)